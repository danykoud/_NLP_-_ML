{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq17bjcJ/algkq+OKih43l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danykoud/Test-/blob/main/NLP_Assign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ30YLcVZ1tX"
      },
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQEFxMLx8Zgd",
        "outputId": "8ac9785c-e17a-4ed2-cf1c-4f06a65b0455"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXHVVh0baALm"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "\n",
        "# Punctuation characters\n",
        "punct = set(string.punctuation)\n",
        "\n",
        "# Morphology rules used to assign unknown word tokens\n",
        "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
        "\n",
        "\n",
        "def get_word_tag(line, vocab): \n",
        "    if not line.split():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "        return word, tag\n",
        "    else:\n",
        "        word, tag = line.split()\n",
        "        if word not in vocab: \n",
        "            # Handle unknown words\n",
        "            word = assign_unk(word)\n",
        "        return word, tag\n",
        "    return None \n",
        "\n",
        "\n",
        "def preprocess(vocab, data_fp):\n",
        "    \"\"\"\n",
        "    Preprocess data\n",
        "    \"\"\"\n",
        "    orig = []\n",
        "    prep = []\n",
        "\n",
        "    # Read data\n",
        "    with open(data_fp, \"r\") as data_file:\n",
        "\n",
        "        for cnt, word in enumerate(data_file):\n",
        "\n",
        "            # End of sentence\n",
        "            if not word.split():\n",
        "                orig.append(word.strip())\n",
        "                word = \"--n--\"\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            # Handle unknown words\n",
        "            elif word.strip() not in vocab:\n",
        "                orig.append(word.strip())\n",
        "                word = assign_unk(word)\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                orig.append(word.strip())\n",
        "                prep.append(word.strip())\n",
        "\n",
        "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
        "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
        "\n",
        "    return orig, prep\n",
        "\n",
        "\n",
        "def assign_unk(tok):\n",
        "    \"\"\"\n",
        "    Assign unknown word tokens\n",
        "    \"\"\"\n",
        "    # Digits\n",
        "    if any(char.isdigit() for char in tok):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Punctuation\n",
        "    elif any(char in punct for char in tok):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Upper-case\n",
        "    elif any(char.isupper() for char in tok):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Nouns\n",
        "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Verbs\n",
        "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Adjectives\n",
        "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Adverbs\n",
        "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "\n",
        "    return \"--unk--\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs0snp40aFin"
      },
      "source": [
        "**Introduction to Dataset**\n",
        "\n",
        "This assignment will use two tagged data sets. One data set (train.pos) will be used for training. The other (test.pos) for testing. The tagged training data has been preprocessed to form a vocabulary (hmv.txt). The words in the vocabulary are words from the training set that were used two or more times. The vocabulary is augmented with a set of 'unknown word tokens'.\n",
        "\n",
        "The training set will be used to create the emission, transmission and tag counts.\n",
        "\n",
        "The test set (test.pos) is read in to create y. This contains both the test text and the true tag. The test set has also been preprocessed to remove the tags to form test_words.txt.\n",
        "\n",
        "A POS tagger will necessarily encounter words that are not in its datasets. A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n",
        "\n",
        "**Task 1** - Training the model: (20 points)\n",
        "Write a program that takes in the training_corpus and returns the three dictionaries mentioned above transition_counts, emission_counts, and tag_counts.\n",
        "\n",
        "**emission_counts:** maps (tag, word) to the number of times it happened.\n",
        "transition_counts: maps (prev_tag, tag) to the number of times it has appeared.\n",
        "tag_counts: maps (tag) to the number of times it has occured.\n",
        "Tip: Make use of defaultdict as a standard Python dictionary throws a KeyError if you try to access an item with a key that is not currently in the dictionary. In contrast, the defauweltdict will create an item of the type of the argument, in this case an integer with the default value of 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6CHo9Lya85L"
      },
      "source": [
        "path1 = \"/content/drive/MyDrive/files/train.pos\"\n",
        "path2= \"/content/drive/MyDrive/files/test.pos\"\n",
        "path3= \"/content/drive/MyDrive/files/hmv.txt\"\n",
        "# Do not change anything in this cell\n",
        "# load in the training corpus\n",
        "with open(path1, 'r') as f:\n",
        "    training_corpus = f.readlines()\n",
        "\n",
        "# read the vocabulary data, split by each line of text, and save the list\n",
        "with open(path3, 'r') as f:\n",
        "    voc_l = f.read().split('\\n')\n",
        "\n",
        "# vocab: dictionary that has the index of the corresponding words\n",
        "vocab = {} \n",
        "\n",
        "# Get the index of the corresponding words. \n",
        "for i, word in enumerate(sorted(voc_l)): \n",
        "    vocab[word] = i       \n",
        "    \n",
        "\n",
        "cnt = 0\n",
        "for k,v in vocab.items():\n",
        "    cnt += 1\n",
        "    if cnt > 20:\n",
        "        break\n",
        "\n",
        "# load in the test corpus\n",
        "with open(path2, 'r') as f:\n",
        "    y = f.readlines()\n",
        "\n",
        "#corpus without tags, preprocessed\n",
        "_, prep = preprocess(vocab,path2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aB9Tm5base3"
      },
      "source": [
        " def create_dictionaries(training_corpus: list, vocab: dict):\n",
        "     \"\"\"Creat the three training dictionaries\n",
        "\n",
        "     Args: \n",
        "        ``training_corpus``: a corpus where each line has a word followed by its tag.\n",
        "        ``vocab``: a dictionary where keys are words in vocabulary and value is an index\n",
        "     Returns: \n",
        "        ``emission_counts``: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        ``transition_counts``: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
        "        ``tag_counts``: a dictionary where the keys are the tags and the values are the counts\n",
        "     \"\"\"\n",
        "\n",
        "     # initialize the dictionaries \n",
        "     emission_counts = defaultdict(int)\n",
        "     transition_counts = defaultdict(int)\n",
        "     tag_counts = defaultdict(int)\n",
        "\n",
        "     # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
        "     prev_tag = '--s--' \n",
        "\n",
        "     \n",
        "     i = 0 \n",
        "     for word_tag in training_corpus:\n",
        "         i += 1\n",
        "\n",
        "         if i % 50000 == 0:\n",
        "             print(f\"word count = {i}\")\n",
        "         # get the word and tag using the get_word_tag  function\n",
        "         word, tag = get_word_tag(word_tag, vocab)\n",
        "\n",
        "        #  increment the transition and emission count by one \n",
        "         transition_counts[(prev_tag, tag)] += 1\n",
        "         emission_counts[(tag, word)] += 1\n",
        "\n",
        "         # Increment the tag count\n",
        "         tag_counts[tag] += 1\n",
        "         prev_tag = tag\n",
        "\n",
        "         ### END CODE HERE ###\n",
        "\n",
        "     return emission_counts, transition_counts, tag_counts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuvLPWGhf_8x",
        "outputId": "97e6a81f-62cd-40e8-d579-943e955ca7c8"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "\n",
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n",
        "states = sorted(tag_counts.keys())\n",
        "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
        "print(\"View these POS tags (states)\")\n",
        "print(states)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word count = 50000\n",
            "word count = 100000\n",
            "word count = 150000\n",
            "word count = 200000\n",
            "word count = 250000\n",
            "word count = 300000\n",
            "word count = 350000\n",
            "word count = 400000\n",
            "word count = 450000\n",
            "word count = 500000\n",
            "word count = 550000\n",
            "word count = 600000\n",
            "word count = 650000\n",
            "word count = 700000\n",
            "word count = 750000\n",
            "word count = 800000\n",
            "word count = 850000\n",
            "word count = 900000\n",
            "word count = 950000\n",
            "Number of POS tags (number of 'states'): 46\n",
            "View these POS tags (states)\n",
            "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6S2ebUgLI1"
      },
      "source": [
        "**Task 2:** Testing the model: (10 points)\n",
        "\n",
        "Implement predict_pos that computes the accuracy of your model.\n",
        "\n",
        "To assign a part of speech to a word, assign the most frequent POS for that word in the training set.\n",
        "\n",
        "Then evaluate how well this approach works. \n",
        "Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same. If so, the prediction was correct!\n",
        "\n",
        "Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvAZRsDmgPQF"
      },
      "source": [
        "def predict_pos(prep, y, emission_counts, vocab, states):\n",
        "    '''\n",
        "    Input: \n",
        "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
        "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
        "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "        states: a sorted list of all possible tags for this assignment\n",
        "    Output: \n",
        "        accuracy: Number of times you classified a word correctly\n",
        "    '''\n",
        "    \n",
        "    # Initialize the number of correct predictions \n",
        "    num_correct_pred = 0\n",
        "    \n",
        "    # Get the (tag, word) tuples, stored as a set\n",
        "    words = set(emission_counts.keys())  \n",
        "    \n",
        "    # Get the number of (word, POS) tuples in the corpus 'y'\n",
        "    total_num_words = len(y)\n",
        "    for word, y_tup in zip(prep, y): \n",
        "\n",
        "        # Split the (word, POS) string into a list of two items\n",
        "        y_tup_split = y_tup.split()\n",
        "        \n",
        "        # Verify that y_tup contain both word and POS\n",
        "        if len(y_tup_split) == 2:\n",
        "            \n",
        "            # Set the true POS label for this word\n",
        "            true_pos_label = y_tup_split[1]\n",
        "\n",
        "        else:\n",
        "            # If the y_tup didn't contain word and POS, go to next word\n",
        "            continue\n",
        "    \n",
        "        count_final = 0\n",
        "        pos_final = ''\n",
        "        \n",
        "        # If the word is in the vocabulary...\n",
        "        if word in vocab:\n",
        "            for pos in states:\n",
        "\n",
        "            ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "                        \n",
        "                # define the key as the tuple containing the POS and word\n",
        "                key = (pos, word)\n",
        "\n",
        "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
        "                if key in emission_counts: # complete this line\n",
        "\n",
        "                # get the emission count of the (pos,word) tuple \n",
        "                    count = emission_counts[key]\n",
        "\n",
        "                    # keep track of the POS with the largest count\n",
        "                    if count > count_final: # complete this line\n",
        "\n",
        "                        # update the final count (largest count)\n",
        "                        count_final = count\n",
        "\n",
        "                        # update the final POS\n",
        "                        pos_final = pos\n",
        "\n",
        "            # If the final POS (with the largest count) matches the true POS:\n",
        "            if pos_final == true_pos_label : \n",
        "                \n",
        "                # Update the number of correct predictions\n",
        "                num_correct_pred += 1\n",
        "            \n",
        "    ### END CODE HERE ###\n",
        "    accuracy = num_correct_pred  / total_num_words\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3pRbFWIlEzL",
        "outputId": "0220a8e6-3372-46e4-e2df-9102308fd4fb"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
        "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of prediction using predict_pos is 0.1077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LzaOgu3lGLq"
      },
      "source": [
        "**Task 3 **\n",
        "\n",
        "**- Hidden Markov Model**: (20 points)\n",
        "\n",
        "Hidden Markov Model: The HMM is one of the most commonly used algorithms in Natural Language Processing. The Markov Model contains a number of states and the probability of transition between those states. In this case, the states are the parts-of-speech. A Markov Model utilizes a transition matrix, A. A Hidden Markov Model adds an observation or emission matrix B which describes the probability of a visible observation when we are in a particular state. In this case, the emissions are the words in the corpus. The state, which is hidden, is the POS tag of that word.\n",
        "\n",
        "The smoothing was done as follows:\n",
        "\n",
        "ğ‘ƒ(ğ‘¡ğ‘–|ğ‘¡ğ‘–âˆ’1)=ğ¶(ğ‘¡ğ‘–âˆ’1,ğ‘¡ğ‘–)+ğ›¼ğ¶(ğ‘¡ğ‘–âˆ’1)+ğ›¼âˆ—ğ‘(3)\n",
        "\n",
        "ğ‘  is the total number of tags\n",
        "ğ¶(ğ‘¡ğ‘–âˆ’1,ğ‘¡ğ‘–)  is the count of the tuple (previous POS, current POS) in transition_counts dictionary.\n",
        "\n",
        "ğ¶(ğ‘¡ğ‘–âˆ’1)  is the count of the previous POS in the tag_counts dictionary.\n",
        "ğ›¼  is a smoothing parameter.\n",
        "\n",
        "Implement the create_transition_matrix below for all tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qwJrNeUlk09"
      },
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    ''' \n",
        "    Input: \n",
        "        alpha: number used for smoothing\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        transition_counts: transition count for the previous word and tag\n",
        "    Output:\n",
        "        A: matrix of dimension (num_tags,num_tags)\n",
        "    '''\n",
        "    # sorted list of all tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    \n",
        "    # length of the list of all tags\n",
        "    len_tags = len(all_tags)\n",
        "    \n",
        "    # Initialize the transition matrix 'A'\n",
        "    A = np.zeros((len_tags,len_tags))\n",
        "    \n",
        "    # Get  previous POS, current POS from  transition tuples \n",
        "    uni_trans_tup = set(transition_counts.keys())\n",
        "    \n",
        "    \n",
        "   \n",
        "    for x in range(len_tags):\n",
        "        for y in range(len_tags):\n",
        "            count = 0\n",
        "            key = (all_tags[x],all_tags[y])\n",
        "            if key in uni_trans_tup: \n",
        "                count = transition_counts[key]\n",
        "\n",
        "            count_prev_tag = tag_counts[all_tags[x]]\n",
        "            \n",
        "            A[x,y] = (count + alpha ) / ( count_prev_tag + alpha * len_tags)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKAVlQsAn1Pq",
        "outputId": "3e5a9cf6-4a81-4fe2-86aa-0f686020c5a0"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "alpha = 0.001\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "# Testing your function\n",
        "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
        "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
        "\n",
        "print(\"View a subset of transition matrix A\")\n",
        "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
        "print(A_sub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A at row 0, col 0: 0.000007040\n",
            "A at row 3, col 1: 0.1691\n",
            "View a subset of transition matrix A\n",
            "              RBS            RP           SYM        TO            UH\n",
            "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
            "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
            "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
            "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
            "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRvfiXszoBhR"
      },
      "source": [
        "Now you will create the B transition matrix which computes the emission probability.\n",
        "\n",
        "You will use smoothing as defined below:\n",
        "\n",
        "ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¡ğ‘–)=ğ¶(ğ‘¡ğ‘–,ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘–)+ğ›¼ğ¶(ğ‘¡ğ‘–)+ğ›¼âˆ—ğ‘(4)\n",
        "\n",
        "- ğ¶(ğ‘¡ğ‘–,ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘–)  is the number of times  ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘–  was associated with  ğ‘¡ğ‘ğ‘”ğ‘–  in the training data (stored in emission_counts dictionary).\n",
        "\n",
        "- ğ¶(ğ‘¡ğ‘–)  is the number of times  ğ‘¡ğ‘ğ‘”ğ‘–  was in the training data (stored in tag_counts dictionary).\n",
        "\n",
        "- ğ‘  is the number of words in the vocabulary\n",
        "\n",
        "- ğ›¼  is a smoothing parameter.\n",
        "\n",
        "The matrix B is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags.\n",
        "\n",
        "Implement the create_emission_matrix below that computes the B emission probabilities matrix. Your function takes in  ğ›¼ , the smoothing parameter, tag_counts, which is a dictionary mapping each tag to its respective count, the emission_counts dictionary where the keys are (tag, word) and the values are the counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNhZQft78orV"
      },
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        alpha: tuning parameter used in smoothing \n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        B: a matrix of dimension (num_tags, len(vocab))\n",
        "    '''\n",
        "    \n",
        "    # get the number of POS tag\n",
        "    num_tags = len(tag_counts)\n",
        "    \n",
        "    # Get a list of all POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    \n",
        "    # Get the total number of unique words in the vocabulary\n",
        "    num_words = len(vocab)\n",
        "    \n",
        "    # Initialize the emission matrix B with places for\n",
        "    # tags in the rows and words in the columns\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "    # from the keys of the emission_counts dictionary\n",
        "    emission_keys = set(list(emission_counts.keys()))\n",
        "       \n",
        "   \n",
        "    for i in range(num_tags): \n",
        "        for j, word in enumerate(vocab): \n",
        "            count = 0\n",
        "            key =  (all_tags[i], word)\n",
        "            \n",
        "            if key in emission_keys:        \n",
        "                count = emission_counts[key] # Get the count of (POS tag, word) from the emission_counts d\n",
        "                \n",
        "            # Get the count of the POS tag\n",
        "            count_tag = tag_counts[all_tags[i]]\n",
        "                \n",
        "            # Apply smoothing and store the smoothed value into the emission matrix B \n",
        "            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)\n",
        "\n",
        "    return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaBEwnaZ-UIu",
        "outputId": "c55b3d41-9c38-4f1e-a86c-ccb9311b5b29"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
        "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
        "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
        "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
        "cols = [vocab[a] for a in cidx]\n",
        "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
        "rows = [states.index(a) for a in rvals]\n",
        "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
        "print(B_sub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View Matrix position at row 0, column 0: 0.000006032\n",
            "View Matrix position at row 3, column 1: 0.000000720\n",
            "              725      adroitly     engineers      promoted       synergy\n",
            "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
            "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
            "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
            "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
            "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
            "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVAB7t1z-gKA"
      },
      "source": [
        "# Task 4 \n",
        "\n",
        "- **Viterbi Algorithm**: (50 points)\n",
        "\n",
        "In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, A and B to compute the Viterbi algorithm. We have decomposed this process into three main steps for you.\n",
        "\n",
        "Initialization - In this part you initialize the best_paths and best_probabilities matrices that you will be populating in feed_forward.\n",
        "Feed forward - At each step, you calculate the probability of each path happening and the best paths up to that point.\n",
        "Feed backward: This allows you to find the best path with the highest probabilities.\n",
        "You will start by initializing two matrices of the same dimension.\n",
        "\n",
        "best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\n",
        "\n",
        "best_paths: A matrix that helps you trace through the best possible path in the corpus.\n",
        "\n",
        "Write a program below that initializes the best_probs and the best_paths matrix.\n",
        "\n",
        "Both matrices will be initialized to zero except for column zero of best_probs.\n",
        "\n",
        "Column zero of best_probs is initialized with the assumption that the first word of the corpus was preceded by a start token (\"--s--\").\n",
        "This allows you to reference the A matrix for the transition probability\n",
        "Here is how to initialize column 0 of best_probs:\n",
        "\n",
        "The probability of the best path going from the start index to a given POS tag indexed by integer  ğ‘–  is denoted by  best_probs[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–] .\n",
        "\n",
        "This is estimated as the probability that the start tag transitions to the POS denoted by index  ğ‘– :  ğ€[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–]  AND that the POS tag denoted by  ğ‘–  emits the first word of the given corpus, which is  ğ[ğ‘–,ğ‘£ğ‘œğ‘ğ‘ğ‘[ğ‘ğ‘œğ‘Ÿğ‘ğ‘¢ğ‘ [0]]] .\n",
        "\n",
        "Conceptually, it looks like this:  best_probs[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–]=ğ€[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–]Ã—ğ[ğ‘–,ğ‘ğ‘œğ‘Ÿğ‘ğ‘¢ğ‘ [0]] \n",
        "\n",
        "In order to avoid multiplying and storing small values on the computer, we'll take the log of the product, which becomes the sum of two logs:\n",
        "\n",
        "ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘Ÿğ‘œğ‘ğ‘ [ğ‘–,0]=ğ‘™ğ‘œğ‘”(ğ´[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–])+ğ‘™ğ‘œğ‘”(ğµ[ğ‘–,ğ‘£ğ‘œğ‘ğ‘ğ‘[ğ‘ğ‘œğ‘Ÿğ‘ğ‘¢ğ‘ [0]] \n",
        "Also, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set  ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘Ÿğ‘œğ‘ğ‘ [ğ‘–,0]=ğ‘“ğ‘™ğ‘œğ‘ğ‘¡(â€²âˆ’ğ‘–ğ‘›ğ‘“â€²)  when  ğ´[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–]==0 \n",
        "\n",
        "So the implementation to initialize  ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘Ÿğ‘œğ‘ğ‘   looks like this:\n",
        "\n",
        "ğ‘–ğ‘“ğ´[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–]<>0:ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘Ÿğ‘œğ‘ğ‘ [ğ‘–,0]=ğ‘™ğ‘œğ‘”(ğ´[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–])+ğ‘™ğ‘œğ‘”(ğµ[ğ‘–,ğ‘£ğ‘œğ‘ğ‘ğ‘[ğ‘ğ‘œğ‘Ÿğ‘ğ‘¢ğ‘ [0]]]) \n",
        "\n",
        "ğ‘–ğ‘“ğ´[ğ‘ ğ‘–ğ‘‘ğ‘¥,ğ‘–]==0:ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘Ÿğ‘œğ‘ğ‘ [ğ‘–,0]=ğ‘“ğ‘™ğ‘œğ‘ğ‘¡(â€²âˆ’ğ‘–ğ‘›ğ‘“â€²)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQxJcYgJ-fPX"
      },
      "source": [
        "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        states: a list of all possible parts-of-speech\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
        "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
        "        corpus: a sequence of words whose POS is to be identified in a list \n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
        "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
        "    '''\n",
        "    # Get the total number of unique POS tags\n",
        "    num_tags = len(tag_counts)\n",
        "    \n",
        "    # Initialize best_probs matrix \n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\n",
        "    \n",
        "    # Initialize best_paths matrix\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
        "    \n",
        "    # Define the start token\n",
        "    s_idx = states.index(\"--s--\")\n",
        "    for i in range(num_tags) : # complete this line\n",
        "        \n",
        "        # Handle the special case when the transition from start token to POS tag i is zero\n",
        "        if A[0,i] == 0:      \n",
        "            best_probs[i,0] = float(\"-inf\")\n",
        "        else: \n",
        "            # Initialize best_probs at POS tag 'i', column 0\n",
        "            best_probs[i,0] = math.log(A[s_idx,i])  +  math.log(B[i,vocab[corpus[0]]])\n",
        "    return best_probs, best_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeZi7PZfELG1",
        "outputId": "34b34c86-2320-47c6-e3cc-76243c15fe44"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "\n",
        "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n",
        "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
        "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_probs[0,0]: -22.6098\n",
            "best_paths[2,3]: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BldUD18XE1u3"
      },
      "source": [
        "Implement the viterbi_forward algorithm and store the best_path and best_prob for every possible tag for each word in the matrices best_probs and best_tags using the pseudo code below.\n",
        "\n",
        "`for each word in the corpus\n",
        "\n",
        "for each POS tag type that this word may be\n",
        "\n",
        "    for POS tag type that the previous word could be\n",
        "\n",
        "        compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.\n",
        "\n",
        "        retain the highest probability computed for the current word\n",
        "\n",
        "        set best_probs to this highest probability\n",
        "\n",
        "        set best_paths to the index 'k', representing the POS tag of the previous word which produced the highest probability `\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqYSUSxBE3vK"
      },
      "source": [
        "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        A, B: The transiton and emission matrices respectively\n",
        "        test_corpus: a list containing a preprocessed corpus\n",
        "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
        "    Output: \n",
        "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
        "    '''\n",
        "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
        "    num_tags = best_probs.shape[0]\n",
        "    for i in range(1, len(test_corpus)): \n",
        "        \n",
        "        # Print number of words processed, every 5000 words\n",
        "        if i % 5000 == 0:\n",
        "            print(\"Words processed: {:>8}\".format(i))\n",
        "        for j in range(num_tags): \n",
        "            # Initialize best_prob for word i to negative infinity\n",
        "            best_prob_i = float(\"-inf\")\n",
        "            # Initialize best_path for current word i to None\n",
        "            best_path_i = None\n",
        "\n",
        "            # For each POS tag that the previous word can be:\n",
        "            for k in range(num_tags): # complete this line\n",
        "            \n",
        "                # Calculate the probability = \n",
        "                # best probs of POS tag k, previous word i-1 + \n",
        "                # log(prob of transition from POS k to POS j) + \n",
        "                # log(prob that emission of POS j is word i)\n",
        "                prob = best_probs[k, i-1] + math.log(A[k,j]) + math.log(B[j, vocab[test_corpus[i]]])\n",
        "\n",
        "                # check if this path's probability is greater than\n",
        "                # the best probability up to and before this point\n",
        "                if prob > best_prob_i:     \n",
        "                    # Keep track of the best probability\n",
        "                    best_prob_i = prob\n",
        "                    \n",
        "                    # keep track of the POS tag of the previous word\n",
        "                    best_path_i = k\n",
        "\n",
        "            # Save the best probability for the \n",
        "            # given current word's POS tag\n",
        "            best_probs[j,i] = best_prob_i\n",
        "            \n",
        "            # Save the unique integer ID of the previous POS tag\n",
        "            best_paths[j,i] = best_path_i\n",
        "\n",
        "    return best_probs, best_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka1yXPz4F3GZ",
        "outputId": "323b1dbe-832e-4b21-ccee-7eeef8140edd"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "# this will take a few minutes to run => processes ~ 30,000 words\n",
        "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words processed:     5000\n",
            "Words processed:    10000\n",
            "Words processed:    15000\n",
            "Words processed:    20000\n",
            "Words processed:    25000\n",
            "Words processed:    30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymAgy_haGIRy"
      },
      "source": [
        "Implement the viterbi_backward algorithm, which returns a list of predicted POS tags for each word in the corpus.\n",
        "\n",
        "Note that the numbering of the index positions starts at 0 and not 1.\n",
        "\n",
        "m is the number of words in the corpus.\n",
        "So the indexing into the corpus goes from 0 to m - 1.\n",
        "\n",
        "Also, the columns in best_probs and best_paths are indexed from 0 to m - 1\n",
        "\n",
        "**In Step 1:**\n",
        "Loop through all the rows (POS tags) in the last entry of best_probs and find the row (POS tag) with the maximum value. Convert the unique integer ID to a tag (a string representation) using the dictionary states.\n",
        "\n",
        "Referring to the three-word corpus described above:\n",
        "\n",
        "z[2] = 28: For the word 'upward' at position 2 in the corpus, the POS tag ID is 28. Store 28 in z at position 2.\n",
        "states(28) is 'RB': The POS tag ID 28 refers to the POS tag 'RB'.\n",
        "\n",
        "pred[2] = 'RB': In array pred, store the POS tag for the word 'upward'.\n",
        "In Step 2:\n",
        "\n",
        "Starting at the last column of best_paths, use best_probs to find the most likely POS tag for the last word in the corpus.\n",
        "Then use best_paths to find the most likely POS tag for the previous word.\n",
        "\n",
        "Update the POS tag for each word in z and in preds.\n",
        "Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.\n",
        "z[1] = best_paths[z[2],2]\n",
        "\n",
        "The small test following the routine prints the last few words of the corpus and their states to aid in debug."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1AfgIEtF9uz"
      },
      "source": [
        "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
        "    '''\n",
        "    This function returns the best path.\n",
        "    \n",
        "    '''\n",
        "    # Get the number of words in the corpus\n",
        "    B= best_paths.shape[1] \n",
        "    # Initialize array arr, same length as the corpus\n",
        "    arr = [None] * B\n",
        "    \n",
        "    # Get the number of unique POS tags\n",
        "    num_tags = best_probs.shape[0]\n",
        "    \n",
        "    # Initialize the best probability for the last word\n",
        "    best_prob_for_last_word = float('-inf')\n",
        "    \n",
        "    # Initialize pred array, same length as corpus\n",
        "    pred = [None] * B\n",
        "    for i in range(num_tags): # complete this line\n",
        "\n",
        "        # If the probability of POS tag at row i\n",
        "        # is better than the previosly best probability for the last word\n",
        "        if best_probs[i,-1] > best_prob_for_last_word: \n",
        "            # Store the new best probability for the last word\n",
        "            best_prob_for_last_word = best_probs[i,-1]\n",
        "            arr[B - 1] = i\n",
        "            \n",
        "    # Convert the last word's predicted POS tag\n",
        "    pred[B - 1] = states[i]\n",
        "    #  Find the best POS tags by walking backward through the best_paths\n",
        "    for j in range(B-1, 0, -1): \n",
        "        # Retrieve the unique integer ID of\n",
        "        # the POS tag for the word at position 'j' in the corpus\n",
        "        pos_tag_for_word_j = best_paths[arr[j], j]\n",
        "#         print(arr[j])\n",
        "        \n",
        "        # retrieve the predicted POS for the word at position j-1 in the corpus\n",
        "        arr[j - 1] = pos_tag_for_word_j\n",
        "        # Get the previous word's POS tag in string form\n",
        "        pred[j - 1] = states[pos_tag_for_word_j]     \n",
        "\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR65slcjJV6N",
        "outputId": "d615de4a-7182-488f-e7af-212a7d64fd7b"
      },
      "source": [
        "# Do not change anything in cell\n",
        "\n",
        "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
        "m=len(pred)\n",
        "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
        "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])\n",
        "\n",
        "print('The third word is:', prep[3])\n",
        "print('Your prediction is:', pred[3])\n",
        "print('Your corresponding label y is: ', y[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction for pred[-7:m-1] is: \n",
            " ['--unk_upper--', '--unk_upper--', '--unk_upper--', '--unk_upper--', '--unk_upper--', '--unk_punct--'] \n",
            " ['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP'] \n",
            "\n",
            "The prediction for pred[0:8] is: \n",
            " ['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP'] \n",
            " ['--unk_upper--', '--unk_upper--', '--unk_punct--', '--unk_upper--', '--unk_upper--', '--unk_upper--', '--unk_upper--']\n",
            "The third word is: --unk_upper--\n",
            "Your prediction is: NNP\n",
            "Your corresponding label y is:  temperature\tNN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fypdCw3OJyRe"
      },
      "source": [
        "def compute_accuracy(pred, y):\n",
        "    '''\n",
        "    Input: \n",
        "        pred: a list of the predicted parts-of-speech \n",
        "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
        "    Output: \n",
        "        \n",
        "    '''\n",
        "    num = 0 #number of times that the prediction nd label match\n",
        "    T = 0 #total number of examples that have valid labels\n",
        "    \n",
        "    # Zip together the prediction and the labels\n",
        "    for prediction, y in zip(pred, y):\n",
        "      \n",
        "        # Split the label into the word and the POS tag\n",
        "        word_postag = y.strip().split(\"\\t\")\n",
        "\n",
        "        if len(word_postag) < 2: \n",
        "            continue \n",
        "        #  separately store  word and tag\n",
        "        word, tag = [item.strip() for item in word_postag]      \n",
        "#         print(tag, prediction) \n",
        "        if tag == prediction:\n",
        "            num+= 1.0\n",
        "        T += 1.0 \n",
        "    return num/T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75WbCH5SM9q1",
        "outputId": "2d5ca282-2a98-4721-f577-9c98309a662e"
      },
      "source": [
        "# Do not change anything in cell\n",
        "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Viterbi algorithm is 0.1071\n"
          ]
        }
      ]
    }
  ]
}